{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7sC9iTuH8nFEkWJwgBZBP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of DCGAN"
      ],
      "metadata": {
        "id": "V4RIWtKsB7dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - DCGAN paper : https://arxiv.org/pdf/1511.06434"
      ],
      "metadata": {
        "id": "pg_bphg4CQAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Architecture guidelines for stable Deep Convolutional GANs (from the paper)\n",
        "\n",
        "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator)\n",
        "- Use batchnorm in both the generator and the discriminator\n",
        "- Remove fully connected hidden layers for deeper architectures\n",
        "- Use ReLU activation in generator for all layers except for the output, which uses Tanh\n",
        "- Use LeakyReLU activation in the discriminator for all layers\n",
        "\n"
      ],
      "metadata": {
        "id": "TgOgmgNaCnL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 - Imports"
      ],
      "metadata": {
        "id": "ORZr8GulCBam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CInK-H1yCC5I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Generator and Discriminator"
      ],
      "metadata": {
        "id": "_QPiIiIECglu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,channels_img,features_d):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "        # Input : N x channels_img x 64 x 64\n",
        "        nn.Conv2d(\n",
        "            channels_img,\n",
        "            features_d,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=1\n",
        "        ), #32x32\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d,features_d*2,4,2,1), #16x16\n",
        "        self._block(features_d*2,features_d*4,4,2,1), #8x8\n",
        "        self._block(features_d*4,features_d*8,4,2,1), #4x4\n",
        "        nn.Conv2d(\n",
        "            features_d*8,\n",
        "            1,\n",
        "            kernel_size=4,\n",
        "            stride=2,\n",
        "            padding=0\n",
        "        ), #1x1 : a single values represents if the image is fake or real\n",
        "        nn.Sigmoid() #to ensure the value is between 0 and 1\n",
        "        )\n",
        "\n",
        "  def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            bias=False #because we're using BatchNorm2d\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.disc(x)"
      ],
      "metadata": {
        "id": "ZsdFNMI0Cixu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator : tbc"
      ],
      "metadata": {
        "id": "BI185o8jFygl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}